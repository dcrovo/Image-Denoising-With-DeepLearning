{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen Final Práctico - Daniel Crovo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision import transforms\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 160\n",
    "IMAGE_WIDTH = 240\n",
    "NOISY_PATH = '../train/'\n",
    "CLEAN_PATH = '../train_cleaned/'\n",
    "VAL_PATH = '../test/'\n",
    "SAVE_PATH = '../preds/'\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "IN_CHANNELS = 1\n",
    "FEATURE_SIZE = 64\n",
    "KERNEL_SIZE = 3\n",
    "FC1_DIM = 128\n",
    "FC2_DIM = 64\n",
    "PADDING = 1\n",
    "BATCH_SIZE = 16\n",
    "PIN_MEMORY =  True\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 200\n",
    "LR = 0.0001\n",
    "psnr = PeakSignalNoiseRatio().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, noisy_image_dir, clean_image_dir, transform=None):\n",
    "        self.noisy_image_dir = noisy_image_dir\n",
    "        self.clean_image_dir = clean_image_dir\n",
    "        self.transform = transform\n",
    "        self.noisy_images = sorted(os.listdir(noisy_image_dir))\n",
    "        self.clean_images = sorted(os.listdir(clean_image_dir))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.clean_images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        clean_img_path = os.path.join(self.clean_image_dir, self.clean_images[index])\n",
    "        noisy_img_path = os.path.join(self.noisy_image_dir, self.noisy_images[index])\n",
    "        clean_image = np.array(Image.open(clean_img_path).convert('L'))\n",
    "        noisy_image = np.array(Image.open(noisy_img_path).convert('L'))\n",
    "\n",
    "        \n",
    "        if self.transform is not None: \n",
    "            transformations = self.transform(image = clean_image, noisy_image = noisy_image)\n",
    "            clean_image = transformations['image']\n",
    "            noisy_image = transformations['noisy_image']\n",
    "\n",
    "        return clean_image, noisy_image\n",
    "\n",
    "class NoisyValData(Dataset):\n",
    "    def __init__(self, val_dir, transform=None):\n",
    "        self.val_dir = val_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(val_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.val_dir, self.images[index])\n",
    "        image = np.array(Image.open(img_path).convert('L'))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformations = self.transform(image = image)\n",
    "            image = transformations['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoaders(clean_dir, noisy_dir, val_dir, batch_size, \n",
    "               train_transform, val_transform, num_workers, \n",
    "               pin_memory):\n",
    "    train_dataset = NoisyTrainData(noisy_image_dir=noisy_dir, clean_image_dir=clean_dir, transform= train_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n",
    "    val_dataset = NoisyValData(val_dir=val_dir, transform = val_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE(nn.Module):\n",
    "    def __init__(self,in_channels, feature_size, kernel_size, padding, fc1_dim, fc2_dim):\n",
    "        super(DAE, self).__init__()\n",
    "        self.inconv2 =feature_size\n",
    "        self.outconv2 = int(feature_size/2)\n",
    "        self.inconv3 = self.outconv2\n",
    "        self.outconv3 = int(self.outconv2/2) \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=feature_size, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(feature_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=self.inconv2, out_channels=self.outconv2, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(self.outconv2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=self.inconv3, out_channels=self.outconv3, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        # Fully connected\n",
    "\n",
    "        # self.flatten = nn.Flatten(start_dim=1)\n",
    "        # self.encoder_linear = nn.Sequential(\n",
    "        #     nn.Linear((self.outconv3*((IMAGE_HEIGHT//2)*(IMAGE_WIDTH//2))), fc1_dim), \n",
    "        #     nn.ReLU(inplace=True), \n",
    "        #     nn.Linear(fc1_dim, fc2_dim),\n",
    "        #     nn.ReLU(True)\n",
    "        # )\n",
    "\n",
    "        # self.decoder_linear = nn.Sequential(\n",
    "        #     nn.Linear(fc2_dim, fc1_dim), \n",
    "        #     nn.ReLU(True),\n",
    "        #     nn.Linear(fc1_dim, self.outconv3*IMAGE_HEIGHT*IMAGE_WIDTH),\n",
    "        #     nn.ReLU(True)\n",
    "        # )\n",
    "        # self.unflatten = nn.Unflatten(dim=1, unflattened_size=(self.outconv3, IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.outconv3, self.inconv3, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(self.inconv3), \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.outconv2, self.inconv2, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(self.inconv2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(feature_size, in_channels, kernel_size=kernel_size, padding=padding, stride =2, output_padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        #print(x.shape)\n",
    "        # #print(x.shape)\n",
    "        # x = self.flatten(x)\n",
    "        # #print(x.shape)\n",
    "\n",
    "        # x = self.encoder_linear(x)\n",
    "        # #print(x.shape)\n",
    "        # x = self.decoder_linear(x)\n",
    "        # #print(x.shape)\n",
    "\n",
    "        # x = self.unflatten(x)\n",
    "        x = self.decoder(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose(\n",
    "    [   A.Resize(height= IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Rotate(limit = 50, p = 1.0), \n",
    "        A.HorizontalFlip(p = 0.3), \n",
    "        A.VerticalFlip(p = 0.1), \n",
    "        A.Normalize(\n",
    "                    mean = [0.0],\n",
    "                    std = [1.0], \n",
    "                    max_pixel_value = 255.0\n",
    "                    ),\n",
    "     ToTensorV2(),],\n",
    "     additional_targets={'noisy_image': 'image' }\n",
    ")\n",
    "val_transforms = A.Compose(\n",
    "    [A.Resize(height= IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "     A.Normalize(\n",
    "                mean = [0.0],\n",
    "                std = [1.0], \n",
    "                max_pixel_value = 255.0\n",
    "                ),\n",
    "     ToTensorV2(),],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = getLoaders(CLEAN_PATH, NOISY_PATH, VAL_PATH, BATCH_SIZE,train_transforms, val_transforms, num_workers=NUM_WORKERS, pin_memory= PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DAE(IN_CHANNELS, FEATURE_SIZE, KERNEL_SIZE, PADDING, FC1_DIM, FC2_DIM).to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(train_loader, model, optimizer, loss_fn, scaler, device, epoch):\n",
    "    p_bar = tqdm(train_loader)\n",
    "    loss_list = []\n",
    "    model.train()\n",
    "    for batch_idx, (clean_img, noisy_img) in enumerate(p_bar):\n",
    "        clean_img = clean_img.float()\n",
    "        noisy_img = noisy_img.float()\n",
    "        clean_img, noisy_img = clean_img.to(device = device), noisy_img.to(device=device)\n",
    "       \n",
    "        #Forward pass\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(noisy_img)\n",
    "            preds = preds.float()\n",
    "            loss = loss_fn(preds, clean_img)\n",
    "\n",
    "\n",
    "        #Backward Pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        loss_list.append(loss.item())\n",
    "        torch.cuda.empty_cache()\n",
    "        #print('Epoch: ', epoch)\n",
    "        p_bar.set_postfix(loss=loss.item())\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(val_loader, model, device, folder):\n",
    "    model.eval()\n",
    "    psnr_list = []\n",
    "    with torch.no_grad():\n",
    "        for val_img in val_loader:\n",
    "            val_img = val_img.float()\n",
    "\n",
    "            val_img = val_img.to(device = device)\n",
    "            preds = model(val_img)\n",
    "            #loss = loss_fn(preds, clean _img)\n",
    "            psnr_value = psnr(preds, val_img)\n",
    "\n",
    "            \n",
    "            psnr_list.append(psnr(preds, val_img))\n",
    "            ##print(f'signal to noise ratio: {psnr_value}')\n",
    "\n",
    "    \n",
    "    return psnr_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds_as_imgs(loader, model, device, folder ): \n",
    "    model.eval()\n",
    "    for idx, (x) in enumerate(loader):\n",
    "        x = x.to(device = device)\n",
    "        with torch.no_grad(): # deshabilitar el cálculo y almacenamiento de gradientes en el grafo computacional de PyTorch\n",
    "            x=x.float()\n",
    "            preds = (model(x))\n",
    "        torchvision.utils.save_image(preds, f'{folder}/y_cleaned_{idx}.png') # #almacenamiento de imagenes procesadas\n",
    "        torchvision.utils.save_image(x.to(torch.float32), f'{folder}/y_noisy_{idx}.png') # almacenamiento de mimagenes con ruido\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR) \n",
    "loss_fn = nn.MSELoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "eval =[]\n",
    "train_loss = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch: ', epoch)\n",
    "    train_loss.append(train_func(train_loader, model, optimizer, loss_fn, scaler, DEVICE, epoch))\n",
    "    eval.append(performance(val_loader, model, DEVICE, SAVE_PATH))\n",
    "    save_preds_as_imgs(val_loader, model, DEVICE, SAVE_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dataiter = iter(train_loader)\n",
    "images, noisy = dataiter._next_data()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[5])\n",
    "nois = np.squeeze(noisy[5])\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(nois, cmap='gray')\n",
    "print(noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
